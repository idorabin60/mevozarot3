# Collocation Extraction on AWS EMR

## Personal Information
*   **Name**: [Your Name]
*   **ID**: [Your ID]
*   **Username**: [Your Username]
*   **Name**: [Partner Name]
*   **ID**: [Partner ID]
*   **Username**: [Partner Username]

## Project Description
This project implements a Collocation Extractor using Hadoop MapReduce on Amazon EMR. It calculates the Log Likelihood Ratio (LLR) for bigrams in the Google N-grams dataset to identify potential collocations.

## How to Run

### Prerequisites
*   Java 8
*   Maven 3.6+
*   Hadoop 3.3.4 (or compatible)

### Compilation
Build the project using Maven to create the executable JAR:
```bash
mvn package
```
This will produce `target/CollocationExtractor-1.0-SNAPSHOT.jar`.

### Execution
Run the JAR on a Hadoop cluster (e.g., EMR) or locally (standalone mode) with the following arguments:

```bash
hadoop jar target/CollocationExtractor-1.0-SNAPSHOT.jar \
  <1-gram_input_path> \
  <2-gram_input_path> \
  <output_path> \
  <stopwords_file_path>
```

**Example:**
```bash
hadoop jar target/CollocationExtractor-1.0-SNAPSHOT.jar \
  s3://datasets.elasticmapreduce/ngrams/books/20090715/heb-all/1gram/data \
  s3://datasets.elasticmapreduce/ngrams/books/20090715/heb-all/2gram/data \
  s3://your-bucket-name/output \
  s3://your-bucket-name/heb-stopwords.txt
```

## MapReduce Steps

The extraction process is divided into 4 sequential MapReduce jobs:

1.  **Step 1: Aggregation & Filtering**
    *   **Input**: Google 1-gram and 2-gram datasets.
    *   **Action**: Filters stop-words (English and Hebrew). Counts occurrences of unigrams and bigrams per decade. Calculates total word count (N) per decade.
    *   **Output**: Compressed step1 output and separate counters (N).

2.  **Step 2: Join C(w1)**
    *   **Input**: Output of Step 1.
    *   **Action**: Aggregates counts for the first word (w1) of the bigram to associate C(w1) with the bigram.
    *   **Output**: Bigrams with C(w1) attached.

3.  **Step 3: Join C(w2) & Calculate LLR**
    *   **Input**: Output of Step 1 (for w2 counts) and output of Step 2 (bigrams with w1 counts).
    *   **Action**: Joins to associate C(w2) with the bigram. Uses N (passed via Configuration), C(w1), C(w2), and C(w1, w2) to calculate the Log Likelihood Ratio (LLR).
    *   **Output**: Bigrams with their LLR scores.

4.  **Step 4: Sorting**
    *   **Input**: Output of Step 3.
    *   **Action**: Sorts the results by LLR in descending order.
    *   **Output**: Top 100 collocations per decade formatted for readability.

## Implementation Notes
*   **Stop Words**: Loaded into the Distributed Cache and used in the Mapper setup phase for efficient filtering.
*   **Decade N Calculation**: Calculated in Step 1 and persisted to a file. The driver reads this file and passes N for each decade to subsequent jobs via the Hadoop Configuration.
*   **Optimizations**: Used strictly typed Writable Comparable keys for efficient sorting and grouping.
